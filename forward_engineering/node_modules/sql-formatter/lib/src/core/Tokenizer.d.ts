import * as regexFactory from './regexFactory';
import { type Token } from './token';
/** Struct that defines how a SQL language can be broken into tokens */
interface TokenizerOptions {
    reservedCommands: string[];
    reservedLogicalOperators?: string[];
    reservedDependentClauses: string[];
    reservedBinaryCommands: string[];
    reservedJoins: string[];
    reservedJoinConditions?: string[];
    reservedKeywords: string[];
    stringTypes: regexFactory.QuoteType[];
    identTypes: regexFactory.QuoteType[];
    variableTypes?: regexFactory.VariableType[];
    openParens?: ('(' | '[' | '{')[];
    closeParens?: (')' | ']' | '}')[];
    positionalParams?: boolean;
    numberedParamTypes?: ('?' | ':' | '$')[];
    namedParamTypes?: (':' | '@' | '$')[];
    quotedParamTypes?: (':' | '@' | '$')[];
    lineCommentTypes?: string[];
    identChars?: regexFactory.IdentChars;
    paramChars?: regexFactory.IdentChars;
    operators?: string[];
    preprocess?: (tokens: Token[]) => Token[];
}
/** Converts SQL language string into a token stream */
export default class Tokenizer {
    private REGEX_MAP;
    private quotedIdentRegex;
    private paramPatterns;
    private input;
    private index;
    private preprocess;
    constructor(cfg: TokenizerOptions);
    private excludePatternsWithoutRegexes;
    /**
     * Takes a SQL string and breaks it into tokens.
     * Each token is an object with type and value.
     *
     * @param {string} input - The SQL string
     * @returns {Token[]} output token stream
     */
    tokenize(input: string): Token[];
    private getWhitespace;
    private getNextToken;
    private matchPlaceholderToken;
    private getEscapedPlaceholderKey;
    private matchQuotedIdentToken;
    private matchReservedWordToken;
    private matchReservedToken;
    private matchToken;
    private match;
}
export {};
